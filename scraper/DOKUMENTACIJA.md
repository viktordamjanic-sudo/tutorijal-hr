# ğŸ“° NOVINA Scraper - Dokumentacija

## SadrÅ¾aj
1. [Uvod](#uvod)
2. [Arhitektura sustava](#arhitektura-sustava)
3. [Instalacija](#instalacija)
4. [KoriÅ¡tenje](#koriÅ¡tenje)
5. [Konfiguracija](#konfiguracija)
6. [Dodavanje novog portala](#dodavanje-novog-portala)
7. [TehniÄke detalje](#tehniÄke-detalje)
8. [RjeÅ¡avanje problema](#rjeÅ¡avanje-problema)
9. [Sigurnost i etika](#sigurnost-i-etika)

---

## Uvod

**NOVINA Scraper** je modularni alat za prikupljanje vijesti s viÅ¡e hrvatskih news portala. Razvijen je s naglaskom na:

- âœ… **Modularnost** - Jednostavno dodavanje novih portala
- âœ… **Brzinu** - Interni API (gdje dostupan) omoguÄ‡uje dohvat 50+ Älanaka u < 3 sekunde
- âœ… **Pouzdanost** - Circuit breaker, retry logika, caching
- âœ… **Neprimjetnost** - Anti-blocking tehnike, human-like ponaÅ¡anje
- âœ… **Kvalitetu podataka** - Puni tekst Älanaka (3,000+ znakova prosjeÄno)

### PodrÅ¾ani portali

| Portal | Metoda | Limit | Opis |
|--------|--------|-------|------|
| **Jutarnji.hr** | JSON API | 200 | Brzo, strukturirani podaci |
| **VeÄernji.hr** | HTML parsing | 50 | BeautifulSoup parsiranje |

### Glavne znaÄajke

| ZnaÄajka | Opis |
|----------|------|
| **Modularna arhitektura** | BaseScraper klasa + portal-specific implementacije |
| **Incremental scraping** | Pamti zadnji ID, ne dohvaÄ‡a duplikate |
| **Circuit Breaker** | Zaustavlja pokuÅ¡aje ako server ne odgovara |
| **Response Cache** | SQLite cache sprjeÄava ponavljane requestove |
| **Proxy Support** | PodrÅ¡ka za rotaciju proxyja |
| **Auto-retry** | Exponential backoff s jitterom |

---

## Arhitektura sustava

### Struktura datoteka

```
scraper/
â”œâ”€â”€ main.py                 # Glavni ulazni point
â”œâ”€â”€ scraper.py             # Backward compatibility wrapper
â”œâ”€â”€ config.py              # Konfiguracijske postavke
â”œâ”€â”€ db.py                  # SQLite baza podataka
â”œâ”€â”€ content_filter.py      # Filtriranje sadrÅ¾aja
â”œâ”€â”€ viewer.py              # Streamlit UI za pregled
â”œâ”€â”€ scrapers/              # Modularni scraper package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py           # BaseScraper apstraktna klasa
â”‚   â”œâ”€â”€ jutarnji.py       # JutarnjiScraper (JSON API)
â”‚   â”œâ”€â”€ vecernji.py       # VecernjiScraper (HTML parsing)
â”‚   â””â”€â”€ factory.py        # Factory za kreiranje scrapera
â”œâ”€â”€ headlines.db           # SQLite baza (generira se)
â”œâ”€â”€ .cache/                # Cache direktorij (generira se)
â”‚   â””â”€â”€ cache.db
â””â”€â”€ requirements.txt       # Python dependencies
```

### Dijagram toka

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   main.py   â”‚â”€â”€â”€â”€â–¶â”‚ Scraper Factory â”‚â”€â”€â”€â”€â–¶â”‚  Portal API     â”‚
â”‚   (CLI)     â”‚     â”‚  (get_scraper)  â”‚     â”‚ (JSON/HTML)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  BaseScraper    â”‚
                    â”‚  - Circuit Br.  â”‚
                    â”‚  - Cache Layer  â”‚
                    â”‚  - Retry Logic  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   SQLite DB     â”‚
                    â”‚  - headlines    â”‚
                    â”‚  - scrape_state â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Instalacija

### Preduvjeti

- Python 3.8+
- pip
- (Opcionalno) Virtual environment

### Koraci instalacije

```bash
# 1. Kloniraj repozitorij
git clone <repo-url>
cd scraper

# 2. Kreiraj virtual environment (preporuÄeno)
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ili: venv\Scripts\activate  # Windows

# 3. Instaliraj ovisnosti
pip install -r requirements.txt

# 4. Testiraj instalaciju
python3 main.py --help
```

### Ovisnosti (requirements.txt)

```
requests==2.32.3          # HTTP requests
beautifulsoup4==4.12.3    # HTML parsing (fallback)
fake-useragent==1.5.1     # Random User-Agent
lxml==5.2.1              # XML/HTML parser
streamlit==1.40.0        # Web UI
pandas==2.2.3            # Data manipulation
```

---

## KoriÅ¡tenje

### Osnovno koriÅ¡tenje

```bash
# Pokreni scraper (svi portali)
python3 main.py

# Samo specifiÄni portal
python3 main.py --portal=jutarnji
python3 main.py --portal=vecernji

# Dry-run (bez spremanja u bazu)
python3 main.py --dry-run

# S punim sadrÅ¾ajem (sporije, za HTML portale)
python3 main.py --portal=vecernji --content
```

### Napredne opcije

```bash
# Postavi limit Älanaka (config.py)
# PORTALS = {"jutarnji": {"limit": 100}}

# Koristi proxije (dodaj u config.py)
# PROXY_LIST = ["http://proxy1:8080", "http://proxy2:8080"]
```

### Streamlit UI

```bash
# Pokreni web suÄelje za pregled
streamlit run viewer.py

# Otvori u pregledniku: http://localhost:8501
```

### Automatsko pokretanje (cron)

```bash
# Uredi crontab
crontab -e

# Dodaj liniju za pokretanje svakih 30 minuta
*/30 * * * * cd /path/to/scraper && python3 main.py >> /var/log/scraper.log 2>&1
```

---

## Konfiguracija

### config.py

```python
import os

# API Token (za slanje na eksterni API)
BEARER_TOKEN = os.getenv("BEARER_TOKEN", "dummy")
API_URL = "https://novina-analysis.novina.workers.dev/import"

# Portali za scraping
PORTALS = {
    "jutarnji": {
        "name": "Jutarnji list",
        "urls": ["https://www.jutarnji.hr/"],
        "limit": 200,              # Max Älanaka po runu
        "fetch_content": False,     # API vraÄ‡a puni sadrÅ¾aj
        "enabled": True,
    },
    "vecernji": {
        "name": "VeÄernji list", 
        "urls": ["https://www.vecernji.hr/"],
        "limit": 50,               # HTML = sporije
        "fetch_content": True,      # Treba dohvatiti sadrÅ¾aj
        "enabled": True,
    }
}

# NaÄin rada filtera:
# - "none"     : Bez filtriranja
# - "blacklist": Filtriraj prema EXCLUDE_PATTERNS (default)
# - "whitelist": Samo ALLOW_PATTERNS
FILTER_MODE = os.getenv("FILTER_MODE", "blacklist")

# Predefinirani filter setovi
ACTIVE_FILTER_SETS = os.getenv("ACTIVE_FILTER_SETS", "").split(",")
ACTIVE_FILTER_SETS = [s.strip() for s in ACTIVE_FILTER_SETS if s.strip()]

# Portal-specific filteri
EXCLUDE_PATTERNS = [
    # Jutarnji.hr
    {"pattern": "/domidizajn/", "type": "startswith", "description": "Dizajn"},
    {"pattern": "/scena/", "type": "startswith", "description": "Scena"},
    {"pattern": "/life/", "type": "startswith", "description": "Lifestyle"},
    {"pattern": "/sportske/", "type": "startswith", "description": "Sport"},
]

VECERNJI_EXCLUDE_PATTERNS = [
    # Vecernji.hr specifiÄni
    {"pattern": "/vijesti/lifestyle/", "type": "startswith"},
    {"pattern": "/sport/", "type": "startswith", "description": "Sport"},
    {"pattern": "/native/", "type": "startswith", "description": "Native ads"},
]

# Proxy lista (opcionalno)
PROXY_LIST = []

# Scraper postavke
REQUEST_DELAY = (1, 3)       # Delay izmeÄ‘u requestova (min, max)
CACHE_TTL = 300              # Cache TTL u sekundama (5 min)
CIRCUIT_BREAKER_THRESHOLD = 5
MAX_WORKERS = 3
```

### Environment Variables

```bash
# Postavi API token
export BEARER_TOKEN="tvoj_token_ovdje"

# Filter mode
export FILTER_MODE="none"           # Bez filtriranja
export FILTER_MODE="blacklist"      # Koristi blacklist
export FILTER_MODE="whitelist"      # Samo whitelist

# Ili u .env datoteci
# echo "BEARER_TOKEN=tvoj_token" > .env
```

---

## Dodavanje novog portala

### 1. Kreiraj scraper klasu

`scrapers/novi_portal.py`:

```python
from typing import List, Tuple
from .base import BaseScraper, Article

class NoviPortalScraper(BaseScraper):
    """
    Scraper za Novi Portal.
    """
    
    PORTAL_NAME = "noviportal"
    BASE_URL = "https://www.noviportal.hr"
    
    def fetch_latest(self, limit: int = 50, since_id: str = None) -> Tuple[List[Article], str]:
        """
        Dohvati najnovije Älanke.
        
        Returns:
            tuple: (list of articles, newest article ID)
        """
        articles = []
        newest_id = since_id
        
        # Implementacija dohvata (API ili HTML parsing)
        # ...
        
        return articles, newest_id
```

### 2. Registriraj u factory

`scrapers/factory.py`:

```python
from .novi_portal import NoviPortalScraper

SCRAPER_REGISTRY = {
    'jutarnji': JutarnjiScraper,
    'vecernji': VecernjiScraper,
    'noviportal': NoviPortalScraper,  # Dodaj ovdje
}
```

### 3. Dodaj u config

`config.py`:

```python
PORTALS = {
    # ... postojeÄ‡i portali ...
    
    "noviportal": {
        "name": "Novi Portal",
        "urls": ["https://www.noviportal.hr/"],
        "limit": 50,
        "fetch_content": True,
        "enabled": True,
    }
}

# Dodaj filtere ako treba
NOVIPORTAL_EXCLUDE_PATTERNS = [
    {"pattern": "/lifestyle/", "type": "startswith"},
]
```

### 4. Testiraj

```bash
python3 main.py --portal=noviportal --dry-run
```

---

## TehniÄke detalje

### 1. Interni API Jutarnjeg

**Endpoint:** `https://www.jutarnji.hr/index.php?option=com_ocm&view=itemlist&layout=latest&format=json`

**Parametri:**
- `limit=N` - Broj Älanaka (default: 100)
- `catid=XXX` - ID kategorije (opcionalno)
- `_cb=timestamp` - Cache-busting

**Odgovor:**
```json
{
  "site": {"url": "...", "name": "..."},
  "items": [
    {
      "id": "15670066",
      "title": "...",
      "link": "/vijesti/...",
      "fulltext": "<p>Puni HTML sadrÅ¾aj...</p>",
      "author": {"name": "...", "link": "..."},
      "publish_up": "2026-02-09 10:00:00"
    }
  ]
}
```

### 2. Incremental Scraping

Scraper pamti `last_article_id` za svaki portal u `scrape_state` tablici:

```sql
SELECT * FROM scrape_state;
-- jutarnji | 15670578 | 2026-02-10 14:02:50
```

Logika:
1. UÄitaj `last_article_id` za portal
2. Dohvati nove Älanke
3. Zaustavi kad naiÄ‘eÅ¡ na `last_article_id`
4. Spremi nove + aÅ¾uriraj `last_article_id`

### 3. Anti-blocking mehanizmi

#### Circuit Breaker
```python
# Ako 5 uzastopnih requestova ne uspije:
# - Prelazi u OPEN stanje
# - Ne Å¡alje nove requestove 60 sekundi
# - Zatim HALF_OPEN - testira jedan request
# - Ako uspije, vraÄ‡a se u CLOSED
```

#### Cache Layer
```python
# Svaki uspjeÅ¡an response se cache-ira u SQLite
# TTL (Time To Live): 300 sekundi (5 min)
# KljuÄ: SHA256(URL + parametri)
```

#### Retry Strategy
```python
# Exponential backoff: 1.5s, 3s, 6s, 12s...
# + Random jitter: Â±0.3s
# Max retries: 5
# Status codes: 429, 500, 502, 503, 504
```

#### Header Rotation
```python
# 3 browser profila se rotiraju:
# 1. Chrome 122 / macOS
# 2. Chrome 120 / Windows  
# 3. Chromium 121 / Linux

# Random User-Agent svaki request
# Random Accept-Language (hr-HR, en-US)
# Random Referer (Google, Facebook, Index.hr...)
```

### 4. Baza podataka

**Tablica: headlines**

| Kolona | Tip | Opis |
|--------|-----|------|
| id | INTEGER PK | Auto-increment ID |
| portal | TEXT | Izvor (npr. "jutarnji", "vecernji") |
| title | TEXT | Naslov Älanka |
| url | TEXT UNIQUE | URL Älanka |
| description | TEXT | Puni tekst Älanka |
| published_at | TEXT | Datum objave |
| author | TEXT | Autor Älanka |
| scraped_at | TEXT | Vrijeme scrape-a |
| sent_to_api | BOOLEAN | Status slanja |

**Tablica: scrape_state**

| Kolona | Tip | Opis |
|--------|-----|------|
| portal | TEXT PK | Naziv portala |
| last_article_id | TEXT | Zadnji viÄ‘eni ID |
| scraped_at | TEXT | Vrijeme zadnjeg scrape-a |

---

## RjeÅ¡avanje problema

### Problem: "Rate limited" (429)

**RjeÅ¡enje:**
- PoveÄ‡aj `request_delay` u config.py: `REQUEST_DELAY=(2, 5)`
- Dodaj proxije u `config.py`
- Smanji `limit` na 25 Älanaka

### Problem: Prazni rezultati

**Dijagnostika:**
```bash
# Testiraj API direktno
curl -s "https://www.jutarnji.hr/index.php?option=com_ocm&view=itemlist&layout=latest&format=json&limit=5" | python3 -m json.tool
```

**RjeÅ¡enje:**
- Ako API ne radi, moÅ¾da su promijenili endpoint
- Provjeri Circuit Breaker status
- ObriÅ¡i cache: `rm -rf .cache/`

### Problem: Duplikati u bazi

**Provjera:**
```bash
sqlite3 headlines.db "SELECT url, COUNT(*) FROM headlines GROUP BY url HAVING COUNT(*) > 1;"
```

**RjeÅ¡enje:**
- Baza koristi `UNIQUE` constraint na `url`
- `INSERT OR IGNORE` sprjeÄava duplikate
- Ako ih ima, obriÅ¡i bazu: `rm headlines.db`

### Problem: Circuit breaker je OPEN

**RjeÅ¡enje:**
- PriÄekaj 60 sekundi (recovery timeout)
- Ili restartaj skriptu

### Problem: Vecernji ne vraÄ‡a sadrÅ¾aj

**RjeÅ¡enje:**
- Provjeri geolocation cookieje: `geoCountry=HR`
- Koristi `--content` flag za puni sadrÅ¾aj
- PoveÄ‡aj `request_delay` - HTML parsiranje je sporije

---

## Sigurnost i etika

### Pravila ponaÅ¡anja

1. **Respect robots.txt** - Kod je iskljuÄen, ali razmisli o ukljuÄivanju
2. **Ne preoptereÄ‡uj server** - Default delay (1-3s) je dovoljan
3. **Cacheiraj odgovore** - Smanjuje optereÄ‡enje
4. **Ne diÅ¾i previÅ¡e threadova** - Max 3 paralelna requesta

### PreporuÄeni limiti

| Resurs | Preporuka | Razlog |
|--------|-----------|--------|
| ÄŒlanaka/run | 50-200 | Ovisno o portalu |
| Interval | 15-30 min | Izbjegni rate limiting |
| Cache TTL | 5 min | Balans svjeÅ¾ine/brzine |
| Retry delay | 1.5s+ | Izgledaj kao Äovjek |

### Legal notice

Scraper je napravljen za **educational/personal use**. Koristi javno dostupne API endpointe i HTML stranice. Ako koristiÅ¡ za komercijalne svrhe:
- ProÄitaj Terms of Service portala
- Kontaktiraj ih za dozvolu
- Ne objavljuj raw sadrÅ¾aj bez dozvole

---

## API Reference

### Factory funkcije

```python
from scrapers import get_scraper, get_supported_portals

# Dohvati scraper po imenu
scraper = get_scraper('jutarnji')

# Lista podrÅ¾anih portala
portals = get_supported_portals()  # ['jutarnji', 'vecernji']
```

### BaseScraper klase

```python
from scrapers import BaseScraper, Article

# Kreiraj custom scraper
class MyScraper(BaseScraper):
    PORTAL_NAME = "myportal"
    BASE_URL = "https://www.myportal.hr"
    
    def fetch_latest(self, limit=50, since_id=None):
        # Implementacija
        articles = []
        newest_id = since_id
        # ... dohvati i parsiraj ...
        return articles, newest_id

# Koristi
scraper = MyScraper()
articles, newest_id = scraper.scrape(max_articles=50)
scraper.print_stats()
scraper.close()
```

### Database API

```python
import db

# Spremi Älanke
db.save_headlines(articles)

# Dohvati neposlane
db.get_unsent_headlines()

# OznaÄi kao poslano
db.mark_sent(articles)

# Statistika
db.get_stats()

# Incremental scraping state
db.get_last_article_id('jutarnji')
db.set_last_article_id('jutarnji', '15670578')
```

---

## Changelog

### v3.0 (2026-02)
- âœ… Modularna arhitektura s BaseScraper klasom
- âœ… PodrÅ¡ka za viÅ¡e portala (Jutarnji, VeÄernji)
- âœ… Factory pattern za jednostavno dodavanje portala
- âœ… Portal-specific konfiguracija i filteri
- âœ… Incremental scraping s scrape_state tablicom

### v2.0 (2026-02)
- âœ… Prelazak na interni API (10x brÅ¾e)
- âœ… Dodan Circuit Breaker
- âœ… Dodan Response Cache
- âœ… Puni tekst Älanaka (3,000+ znakova)

### v1.0 (2024)
- ğŸ“ Inicijalna verzija
- ğŸ“ HTML parsing s BeautifulSoup
- ğŸ“ Basic retry logika

---

## Autor i kontakt

Razvijeno za **NOVINA** projekt.

Za pitanja i prijedloge:
- Email: [tvoj-email]
- GitHub Issues: [repo-url]/issues

---

## Licenca

MIT License - Slobodno koristi, modificiraj i distribuiraj.

**Napomena:** Odgovornost za koriÅ¡tenje je na korisniku. PoÅ¡tuj pravila i ne zloupotrebljavaj.
